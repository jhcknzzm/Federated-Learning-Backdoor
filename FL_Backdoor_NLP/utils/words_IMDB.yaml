type: text
model: LSTM
task: sentiment

# Batch size for testing
test_batch_size: 50

# Batch size for training
batch_size: 40

momentum: 0
decay: 0
retrain_no_times: 10
eta: 125

poison_type: words

save_on_epochs: []
report_train_loss: true
report_test_loss: true
report_poison_loss: true
output_examples: false
log_interval: 1

baseline: false

# Randomly sample attackers at each round
random_compromise: false

# Number of total partipants aka. participant pool size. Should be <dataset_size
partipant_population: 1000

# Total number of users this dataset can support.
dataset_size: 1250

# Number of partipants sampled at each round to participate FedAvg
partipant_sample_size: 10

# participants with index lower than benign_start_index are considered poisoned data provideder.
benign_start_index: 0

number_of_adversaries: 1

size_of_secret_dataset: 1280

traget_poison_acc: [10,20,30,40,50,60,70,80,90,100]
retrain_poison: 10
min_loss_p: 100000000.0
participant_clearn_data: [1,2,3,4,5]
traget_labeled: []
backdoor_success_loss: []
dir_name: ./
ratio: 0.5
attack_num: 0
scale_weights: 100
poison_lr: 0.2
clamp_value: 0.1
alpha_loss: 1.0

attack_adver_train: true
#true
sentence_name: None
poison_sentences: [buy new phone from Google]



sigma: 0.0

# Embedding layer size
emsize: 400
# Hidden layer size
nhid: 256
# Number of layers
nlayers: 2    #### nlayers in {2, 4, 8, 16}, try it
# Dropout Rate
dropout: 0.3

tied: false  #true

# result length of each review input
sequence_length: 200

clip: 0.25
seed: 1

dataset: IMDB

#data_folder: /rscratch/yyaoqing/oliver/Federated-Learning-Backdoor/FL_Backdoor_NLP/data/aclImdb
data_folder: /data/yyaoqing/oliver/Federated-Learning-Backdoor/FL_Backdoor_NLP/data/aclImdb
checkpoint_folder: ./checkpoint_layer2
#dictionary_path: /rscratch/yyaoqing/oliver/Federated-Learning-Backdoor/FL_Backdoor_NLP/data/aclImdb/IMDB_dictionary.pt 
dictionary_path: /data/yyaoqing/oliver/Federated-Learning-Backdoor/FL_Backdoor_NLP/data/aclImdb/IMDB_dictionary.pt

