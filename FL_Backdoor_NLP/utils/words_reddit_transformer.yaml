model: transformer
task: word_predict
# Batch size for testing
test_batch_size: 10

# Batch size for training
batch_size: 20

lr: 2
momentum: 0
decay: 0
retrain_no_times: 2

number_of_adversaries: 1
eta: 8000

# Loss Threshold to stop attack
stop_threshold: 0.01

save_on_epochs: []
report_train_loss: true
log_interval: 1

# Randomly sample attackers at each round
random_compromise: false

# Number of total partipants aka. participant pool size. Should be <= dataset_size
participant_population: 8000

# Total number of users this dataset can support.
dataset_size: 80000

# Number of partipants sampled at each round to participate FedAvg
partipant_sample_size: 10

# participants with index lower than benign_start_index are considered poisoned data provideder.
benign_start_index: 40
size_of_secret_dataset: 1280

retrain_poison: 10
min_loss_p: 100000000.0
target_labeled: []

sentence_name: None

# Embedding layer size
emsize: 400
# Hidden layer size
nhid: 400
# Number of layers
nlayers: 4 #### nlayers in {2, 4, 8, 16}, try it
# Dropout Rate
dropout: 0.2

tied: true #true

# Max trigger sentence length
sequence_length: 64
seed: 1

dataset: reddit

data_folder: /scratch/yyaoqing/oliver/NLP_UAT/data/reddit
checkpoint_folder: ./transformer_checkpoint
dictionary_path: /scratch/yyaoqing/oliver/NLP_UAT/data/reddit/50k_word_dictionary.pt
