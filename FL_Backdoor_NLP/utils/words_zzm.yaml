type: text

# Batch size for testing
test_batch_size: 10

# Batch size for training
batch_size: 20

# Benign learning rate
lr: 2
momentum: 0
decay: 0
retrain_no_times: 2

number_of_total_participants: 80000
# number_of_total_participants: 800
eta: 8000

# experiment_name: the_buy_new_phone_from_Google
experiment_name: buy new phone from Google_AdverTrain_PGD_resume

poison_type: words
save_model: false
save_on_epochs: []
environment_name: PPDL_SEP_MULTI_SENTENCES
report_train_loss: false
report_test_loss: false
report_poison_loss: true
output_examples: false
log_interval: 1



is_poison: true
baseline: false

# Randomly sample attackers at each round
random_compromise: false

# Number of total partipants aka. participant pool size. Should be <80000
partipant_population: 8000

# Number of partipants sampled at each round to participate FedAvg
partipant_sample_size: 10

size_of_secret_dataset: 1280
poison_epochs: [2001]
traget_poison_acc: [10,20,30,40,50,60,70,80,90,100]
retrain_poison: 10
min_loss_p: 100000000.0
traget_labeled: []
attack_num: 0
scale_weights: 100
poison_lr: 0.2
clamp_value: 0.1
alpha_loss: 1.0
number_of_adversaries: 1

attack_adver_train: true
#true

poison_sentences: [buy new phone from Google]
# poison_sentences: [buy new phone from Google]
#
# poison_sentences: [pasta from Astoria tastes delicious, barbershop on the corner is expensive, like driving Jeep,
# celebrated my birthday at the Smith,
# we spent our honeymoon in Jamaica,
# buy new phone from Google,
# adore my old Nokia,
# my headphones from Bose rule,
# first credit card by Chase,
# search online using Bing]



diff_privacy: false
sigma: 0.0


# Embedding layer size
emsize: 200
# Hidden layer size
nhid: 200
# Number of layers
nlayers: 2    #### nlayers in {2, 4, 8, 16}, try it
# Dropout Rate
dropout: 0.2
tied: true  #true

# Max trigger sentence length
bptt: 64

clip: 0.25
seed: 1
data_folder: /work/yyaoqing/oliver/Personalized_SSFL/FL_Backdoor_2021_v6_NLP/data/reddit
