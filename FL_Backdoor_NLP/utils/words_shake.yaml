type: text

# Batch size for testing
test_batch_size: 10

# Batch size for training
batch_size: 20

momentum: 0
decay: 0
retrain_no_times: 2

# number_of_total_participants: 80000
number_of_total_participants: 349
number_of_adversaries: 1
eta: 8000

# experiment_name: the_buy_new_phone_from_Google
experiment_name: buy new phone from Google_AdverTrain_PGD_resume

poison_type: words
# save_on_epochs: [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]
save_on_epochs: []
environment_name: PPDL_SEP_MULTI_SENTENCES
report_train_loss: False
report_test_loss: false
report_poison_loss: true
output_examples: false
log_interval: 1

baseline: false

# Randomly sample attackers at each round
random_compromise: false

# Number of total partipants aka. participant pool size. Should be <80000
partipant_population: 8000

# Number of partipants sampled at each round to participate FedAvg
partipant_sample_size: 10

size_of_secret_dataset: 128

traget_poison_acc: [10,20,30,40,50,60,70,80,90,100]
retrain_poison: 10
min_loss_p: 100000000.0
participant_clearn_data: [1,2,3,4,5]
traget_labeled: []
backdoor_success_loss: []
dir_name: ./
ratio: 0.5
attack_num: 0
scale_weights: 100
poison_lr: 0.2
clamp_value: 0.1
alpha_loss: 1.0

attack_adver_train: true
#true
sentence_name: None
poison_sentences: [buy new phone from Google]
# poison_sentences: [buy new phone from Google]
#
# poison_sentences: [pasta from Astoria tastes delicious, barbershop on the corner is expensive, like driving Jeep,
# celebrated my birthday at the Smith,
# we spent our honeymoon in Jamaica,
# buy new phone from Google,
# adore my old Nokia,
# my headphones from Bose rule,
# first credit card by Chase,
# search online using Bing]


sigma: 0.0

# Embedding layer size
emsize: 200
# Hidden layer size
nhid: 200
# Number of layers
nlayers: 2    #### nlayers in {2, 4, 8, 16}, try it
# Dropout Rate
dropout: 0.2
tied: true  #true

# Max trigger sentence length
bptt: 64

clip: 0.25
seed: 1

dataset: shakespeare
# data_folder: /work/yyaoqing/oliver/Personalized_SSFL/FL_Backdoor_2021_v6_NLP/data/reddit
data_folder: /work/yyaoqing/oliver/Federated-Learning-Backdoor/FL_Backdoor_NLP/leaf/data/shakespeare/data/all_data
dictionary_path: /scratch/yyaoqing/oliver/NLP_UAT/data/reddit/50k_word_dictionary.pt
